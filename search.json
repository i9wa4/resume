[
  {
    "objectID": "details/07-pivot.html",
    "href": "details/07-pivot.html",
    "title": "PIVOT株式会社",
    "section": "",
    "text": "Table 1: 会社概要・雇用形態 (2025年時点)\n\n\n\n\n\n\n\n\n\n事業内容\n経済コンテンツサービス開発\n\n\n資本金\n1億円\n\n\n従業員数\n約50名 (インターン含む)\n\n\n上場／非上場\n非上場\n\n\n雇用形態\n業務委託\n\n\n勤務地\nフルリモート (居住地 北海道札幌市)\n\n\n所属",
    "crumbs": [
      "職務経歴 詳細",
      "PIVOT株式会社"
    ]
  },
  {
    "objectID": "details/02-sld.html",
    "href": "details/02-sld.html",
    "title": "新光商事LSIデザインセンター株式会社",
    "section": "",
    "text": "Table 1: 会社概要・雇用形態 (2022年時点)\n\n\n\n\n\n\n\n\n\n事業内容\n組込ソフトウェア開発、ASIC/FPGA 開発\n\n\n資本金\n8000万円\n\n\n売上高\n16.7億円\n\n\n従業員数\n83名\n\n\n上場／非上場\n非上場\n\n\n勤務地\n北海道札幌市2020年3月以降は週に約3回リモートワーク実施\n\n\n雇用形態\n正社員\n\n\n所属\n[2018/08 - 2022/03] ソフト開発一部 開発二課",
    "crumbs": [
      "職務経歴 詳細",
      "新光商事LSIデザインセンター株式会社"
    ]
  },
  {
    "objectID": "details/02-sld.html#親会社向け車両通信プロトコル解説講義",
    "href": "details/02-sld.html#親会社向け車両通信プロトコル解説講義",
    "title": "新光商事LSIデザインセンター株式会社",
    "section": "1. 親会社向け車両通信プロトコル解説講義",
    "text": "1. 親会社向け車両通信プロトコル解説講義\n\n1.1. 期間\n\n2022/02 - 2022/03\n\n\n\n1.2. 担当業務\n\n車両組込システムの通信プロトコル (CAN) の説明資料作成\n組込システムへのCAN通信実装\n上記を利用した講義実施\n\n\n\n1.3. 担当フェーズ\n\nなし\n\n\n\n1.4. 開発環境\n\nC\nアセンブラ\nRenesas CS+ V8.05.00\nRH850/C1M-A1\n\n\n\n1.5. 規模・役割\n\n2名\nメンバー",
    "crumbs": [
      "職務経歴 詳細",
      "新光商事LSIデザインセンター株式会社"
    ]
  },
  {
    "objectID": "details/02-sld.html#噴霧器システム開発",
    "href": "details/02-sld.html#噴霧器システム開発",
    "title": "新光商事LSIデザインセンター株式会社",
    "section": "2. 噴霧器システム開発",
    "text": "2. 噴霧器システム開発\n\n2.1. 期間\n\n2021/06 - 2022/01\n\n\n\n2.2. 担当業務\n\nプロトタイプ向け汎用インバータ制御開発\n顧客インバータ制御開発\n噴霧器の操作器開発\n\n\n\n2.3. 実績・取り組み\n\n顧客要望で解析用グラフ資料作成業務が発生した際に、自主的に習得していたPythonを用いて満足いただける資料を作成できました。\n\n\n\n2.4. 担当フェーズ\n\n要件分析\n基本設計\n詳細設計\n開発\n\n\n\n2.5. 開発環境\n\nC\nアセンブラ\nRenesas CS+ V8.05.00\nRL78/G1F\nRL78/G12\n\n\n\n2.6. 規模・役割\n\n4名\nメンバー",
    "crumbs": [
      "職務経歴 詳細",
      "新光商事LSIデザインセンター株式会社"
    ]
  },
  {
    "objectID": "details/02-sld.html#車両緊急通報システムのマイコン移植",
    "href": "details/02-sld.html#車両緊急通報システムのマイコン移植",
    "title": "新光商事LSIデザインセンター株式会社",
    "section": "3. 車両緊急通報システムのマイコン移植",
    "text": "3. 車両緊急通報システムのマイコン移植\n\n3.1. 期間\n\n2020/10 - 2021/05\n\n\n\n3.2. 担当業務\n\nマイコン移植による故障懸念点の洗い出し\nマイコンのリソース割当検討\n各種タイマ機能とシリアル通信機能の移植\nテスト仕様書作成\nテスト実施\n\n\n\n3.3. 実績・取り組み\n\n5名が各々実施していた30以上のテスト仕様書のフォーマット整備をExcel VBAで自動化することで全員の負担を軽減し、特に経験の浅い1名の負担軽減に大きく貢献できました。\n\n\n\n3.4. 担当フェーズ\n\n要件分析\n基本設計\n詳細設計\n開発\nテスト\n\n\n\n3.5. 開発環境\n\nC\nアセンブラ\nGHS MULTI V2020.1.5\nQAC 7.2.3\nRH850/F1K\n\n\n\n3.6. 規模・役割\n\n9名\nメンバー",
    "crumbs": [
      "職務経歴 詳細",
      "新光商事LSIデザインセンター株式会社"
    ]
  },
  {
    "objectID": "details/02-sld.html#自動車トラクション制御ファームウェアの無線更新機能の開発",
    "href": "details/02-sld.html#自動車トラクション制御ファームウェアの無線更新機能の開発",
    "title": "新光商事LSIデザインセンター株式会社",
    "section": "4. 自動車トラクション制御ファームウェアの無線更新機能の開発",
    "text": "4. 自動車トラクション制御ファームウェアの無線更新機能の開発\n\n4.0.1. 期間\n\n2020/04 - 2020/09\n\n\n\n4.1. 担当業務\n\nバックアップ・更新処理の設計開発\nカバレッジテスト・単体テスト・結合テストの仕様書作成\nテスト実施\n\n\n\n4.2. 担当フェーズ\n\n要件分析\n基本設計\n詳細設計\n開発\nテスト\n\n\n\n4.3. 開発環境\n\nC\nアセンブラ\nGHS MULTI V2013.5.5\nQAC 8.1.1J\nwinAMS v6.3.1\nRH850/C1M\n\n\n\n4.4. 規模・役割\n\n8名\nメンバー",
    "crumbs": [
      "職務経歴 詳細",
      "新光商事LSIデザインセンター株式会社"
    ]
  },
  {
    "objectID": "details/02-sld.html#自動車トラクション制御開発用ファームウェアの更新機能開発",
    "href": "details/02-sld.html#自動車トラクション制御開発用ファームウェアの更新機能開発",
    "title": "新光商事LSIデザインセンター株式会社",
    "section": "5. 自動車トラクション制御開発用ファームウェアの更新機能開発",
    "text": "5. 自動車トラクション制御開発用ファームウェアの更新機能開発\n\n5.1. 期間\n\n2019/10 - 2020/03\n\n\n\n5.2. 担当業務\n\nトラクションモータ開発基板側の通信機能・自身のROM書き換え機能の設計開発\nPCアプリ含めたシステムのテスト仕様作成\nテスト実施\n\n\n\n5.3. 担当フェーズ\n\n要件分析\n基本設計\n詳細設計\n開発\nテスト\n\n\n\n5.4. 開発環境\n\nC\nアセンブラ\nGHS MULTI V2015.1.7\nRH850/C1M-A1\n\n\n\n5.5. 規模・役割\n\n6名\nメンバー",
    "crumbs": [
      "職務経歴 詳細",
      "新光商事LSIデザインセンター株式会社"
    ]
  },
  {
    "objectID": "details/index.html",
    "href": "details/index.html",
    "title": "職務経歴 詳細",
    "section": "",
    "text": "就業形態に関わらず就業時期の新しい順に記載しております。\n\n\n\n\n\n\n\n\n\nPeriod\n\n\n\nCategories\n\n\n\nTitle\n\n\n\nDescription\n\n\n\n\n\n\n\n\n2025/06 - 現在\n\n\n業務委託\n\n\nPIVOT株式会社\n\n\nデータエンジニア・MLOps エンジニア \n\n\n\n\n\n\n2024/11 - 現在\n\n\n正社員\n\n\n株式会社GENDA\n\n\nデータエンジニア・MLOps エンジニアとして以下に従事 - データパイプライン全体の変更対応 - データマート整備 - ダッシュボード作成・運用 - AWS から Databricks への機械学習開発基盤移行対応 - CI 導入など機械学習基盤の整備 \n\n\n\n\n\n\n2024/04 - 2024/10\n\n\n正社員\n\n\n株式会社hacomono\n\n\nデータエンジニアとして以下に従事 - 自社環境・顧客環境の DWH 構築・運用 - 自社データ基盤の運用改善・機能追加 \n\n\n\n\n\n\n2022/04 - 2024/03\n\n\n正社員\n\n\n株式会社シイエヌエス北海道\n\n\nデータエンジニア・MLOps エンジニアとして以下に従事 - データパイプライン構築 - データマート整備 - 機械学習ワークフロー構築 \n\n\n\n\n\n\n2018/08 - 2022/03\n\n\n正社員\n\n\n新光商事LSIデザインセンター株式会社\n\n\n組込エンジニアとして以下に従事 - 車載マイコンの機能開発 - インバータやモータの制御開発 \n\n\n\n\n\n\n2016/04 - 2018/07\n\n\n正社員\n\n\nオークマ株式会社\n\n\nWindows アプリエンジニアとして以下に従事 - Windows 用 CAD/CAM アプリ開発の要件定義・テスト \n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "details/05-genda.html",
    "href": "details/05-genda.html",
    "title": "株式会社GENDA",
    "section": "",
    "text": "Table 1: 会社概要・雇用形態 (2024年時点)\n\n\n\n\n\n\n\n\n\n事業内容\nグループ各社の事業成長の支援および経営管理\n\n\n資本金\n197億6,465万円（資本剰余金含む）\n\n\n売上高\n連結：556億円（2024年1月期）\n\n\n従業員数\n連結：12,056名（2024年8月31日時点）\n\n\n上場／非上場\n上場\n\n\n雇用形態\n正社員\n\n\n勤務地\nフルリモート (居住地 北海道札幌市)年に数回汐留本社へ出社\n\n\n所属\n[2024/11 - 現在] IT戦略部 データチーム",
    "crumbs": [
      "職務経歴 詳細",
      "株式会社GENDA"
    ]
  },
  {
    "objectID": "details/05-genda.html#データ-カラオケ事業会社のデータ利活用推進",
    "href": "details/05-genda.html#データ-カラオケ事業会社のデータ利活用推進",
    "title": "株式会社GENDA",
    "section": "1. [データ] カラオケ事業会社のデータ利活用推進",
    "text": "1. [データ] カラオケ事業会社のデータ利活用推進\n\n1.1. 概要\n\n様々な意思決定をデータに基づいて行うために必要なデータ基盤の構築運用とデータ活用方法の提案\n\n\n\n1.2. 期間\n\n2024/11 - 現在\n\n\n\n1.3. 規模・役割\n\n役割：リーダー\n規模：4名\n\nデータエンジニアリング全般：2名 (自分含む)\n\n内訳\n\nデータパイプライン構築：メンバー1名\ndbt によるデータマート構築：私1名\nダッシュボード作成：私1名\n\n\n事業会社側役員との折衝等担当 (BizDev)：1名\n事業会社側経理担当：1名\n\n\n\n\n1.4. 担当業務\n\nPOS データやアプリDBのデータを Snowflake に集約するデータパイプライン構築・運用\n\n補足：Snowflake に転送後 Databricks からテーブルを参照しています\n\n用途に応じた dbt によるデータマート構築\nデータ基盤上のデータを利用したダッシュボード作成・運用\nデータ基盤 (Databricks) やデータ利活用方法の啓蒙\n\n\n\n1.5. 機能開発・実装詳細\n\nデータパイプライン構築\n\nMWAA を用いて事業会社側の SFTP サーバ上の CSV ファイルをダウンロードし Snowflake のテーブルへデータを追加する\nデータパイプライン全体を Terraform で管理している\nあるいは直接受領したファイルを Snowflake へ手動追加する\n\ndbt によるデータマート構築\n\nDatabricks で dbt Core ジョブを実行し Databricks 上にデータマートを構築する\n\nダッシュボード作成\n\nDatabricks のダッシュボード機能を利用する\n\n\n\n\n1.6. 目的・背景\n\n開発が生じる要因\n\nデータパイプライン構築\n\nM&A によりグループインした企業の POS データを新たに取り込む\n目標数値など連携対象が増加した際に適宜取り込む\nIPコラボの情報などのマスターデータを営業サイドから受領する\n\ndbt によるデータマート構築\n\nダッシュボード作成に必要なデータ集計処理時間の短縮のために dbt によるバッチ処理に切り出す\n\nダッシュボード作成\n\n料金体系の変更など施策の効果を監視するために可視化したいデータの要望を受ける\n\n\n\n\n\n1.7. 課題\n\n人員不足による開発遅延\n\n私がデータ転送部分の開発・改修を自分の手で直接実施はせず他メンバーに任せるという業務分担状況で、なおかつグループインする企業が増え続けている都合でデータ転送部分を担うデータエンジニアの手が回りづらい状況が続いていました。\n経営目線で求められるダッシュボードを作成する上で必要なデータへの知見や技術をもった人間が私しかいないため私がボトルネックになっています。\n\n\n\n\n1.8. 工夫した点\n\n\n私がデータ転送部分の開発・改修に深くコミットしていないという業務分担状況で、なおかつグループインする企業が増え続けている都合でデータ転送部分を担うデータエンジニアの手が回りづらい状況が続いている\n\n\n後段の開発作業の工数を削減する目的で、現状の実装から最小限の変更で対応できる要件となるよう事業会社側と交渉をまとめました。 実装作業こそ自分で実施しないものの、実装内容を100%理解できているため業務の切り出しを最適な形で行うことができました。\n\n\n経営目線で求められるダッシュボードを作成する上で必要なデータへの知見や技術をもった人間が私しかいないため私がボトルネックになっています。\n\n\n現状も続いている課題であって大きな成果はまだ出せていません。 現在事業会社側では全社的にトップダウンな意思決定を伴いつつDX推進中になるため、事業会社側のメンバーの工数の確保をトップダウンで実施してもらい、その時間を活用して私の手で人材育成を行う流れを検討しています。 生成AIを補助に利用する方法などを提示しつつできる形で関与できるメンバーを増やしていければと思います。\n\n\n1.9. 成果\n\nダッシュボード\n\n事業会社側の経理部門により Excel で毎日作成されていた売上速報をダッシュボードにより完全自動化できました。\n週次で実施している全国のエリア統括者と役員の参加する営業会議向けに作成していた週報の作成をほぼ自動化できました。\n事業会社側の担当者から追跡したいデータに関する要望が出てくるようになり、データドリブンな意思決定をできる体質に一歩近付きました。\n\nデータ基盤\n\nマーケティング施策の効果検証に利用するデータを揃えることで効果的なマーケティング施策立案に貢献しました。\n\n\n\n\n1.10. 担当フェーズ\n\nヒアリングや提案による以下の要件整理\n\nデータパイプライン改修\nダッシュボード新規作成・項目追加\n\n開発指揮\n\nデータパイプライン改修\n\n開発作業\n\ndbt によるデータマート構築\nダッシュボード新規作成・項目追加\n\n運用作業\n\nデータパイプライン不通時の原因調査\nデータマートやダッシュボードの障害解消\n\n\n\n\n1.11. 開発環境\n\nGitHub\nAWS\n\nMWAA\n\nSnowflake\nDatabricks\ndbt Core\nTerraform",
    "crumbs": [
      "職務経歴 詳細",
      "株式会社GENDA"
    ]
  },
  {
    "objectID": "details/05-genda.html#データ-ゲームセンター事業会社のipマスターデータ作成支援",
    "href": "details/05-genda.html#データ-ゲームセンター事業会社のipマスターデータ作成支援",
    "title": "株式会社GENDA",
    "section": "2. [データ] ゲームセンター事業会社のIPマスターデータ作成支援",
    "text": "2. [データ] ゲームセンター事業会社のIPマスターデータ作成支援\n※IP：知的財産 (Intellectual Property)。ゲームやアニメのタイトルなどを指しています。\n\n2.1. 概要\n\nIP名、IP・キャラクターの関係性、景品・IP・キャラクターの関係性を整理したマスターデータを作成する\n\n\n\n2.2. 期間\n\n2025/04 - 現在\n\n\n\n2.3. 規模・役割\n\n役割：リーダー\n規模：4名\n\nマスターデータ設計・システム設計：私1名\nkintone アプリ PoC 担当：1名\n事業会社側メンバーとの調整担当兼 kintone アプリ実装担当：1名\nデータパイプライン構築：1名\n\n\n\n\n2.4. 担当業務\n\nデータ入力用の kintone アプリ設計\nkintone アプリからデータ基盤への連携に必要なデータパイプライン設計\nkintone アプリの運用方法検討\n\n\n\n2.5. 機能開発・実装詳細\n\nkintone アプリ\n\nIP名、IP・キャラクターの関係性、景品・IP・キャラクターの関係性を表すマスターデータへのデータ追加・修正を行う\n\nデータパイプライン\n\nkintone アプリから Snowflake へのデータ転送を行う\nデータパイプライン全体を Terraform で管理する\n\n\n\n\n2.6. 目的・背景\n\nIPやキャラクターを包括的に管理する仕組みが社内外ともに存在せず、景品選定も担当者の経験則に依存しており改善が求められる状況でした。\n景品の売上実績やIPの流行に関する情報をデータとして蓄積していき更に景品選定に活用するサイクルを作っていきたい狙いがあります。\n\n\n\n2.7. 課題\n\n以下の要求があり、入力からデータ基盤への連携までどのようなシステムが最適か検討する必要がありました。\n\n景品を卸す業者に一次情報を入力してもらう kintone アプリの導入が進められている最中であった\n責任分掌の観点からデータの修正や管理を調達部門に自律的に実施してもらう必要がある\n毎日数十件のデータ入力が必要でありデータ入力担当者の負担を極力軽減させたい\n\n当初利便性を考慮して Notion によるデータ入力を検討していました。Notion では誤入力が起こりやすいという欠点があるため Notion 上の仮のマスターデータと、データ基盤上に別途真のマスターデータを用意しておき、レビューを行った後に両者を同期するという運用を検討しました。\n\nこの構成の場合レビュー工程が都度入ることで入力担当者の負担が大きくなることが予想されました。\n\n\n\n\n2.8. 工夫した点\n\nkintone アプリの仕様を調査していくうちに入力データへのバリデーションを設定でき誤入力を防げることが分かりました。これにより当初考慮していなかったマスターデータ管理用社内 kintone アプリの導入を決めました。\n\nこの方向転換によって課題をほぼ全て解消できました。\n\n\n\n\n2.9. 成果\n\n開発中\n\n\n\n2.10. 担当フェーズ\n\nマスターデータの設計\nシステム全体の要件定義・設計\n\n\n\n2.11. 開発環境\n\nデータ入力アプリ\n\nkintone\n\nデータ基盤\n\nGitHub\nAWS\n\nMWAA\n\nSnowflake\nTerraform",
    "crumbs": [
      "職務経歴 詳細",
      "株式会社GENDA"
    ]
  },
  {
    "objectID": "details/05-genda.html#mlops-機械学習開発運用環境の改善",
    "href": "details/05-genda.html#mlops-機械学習開発運用環境の改善",
    "title": "株式会社GENDA",
    "section": "3. [MLOps] 機械学習開発運用環境の改善",
    "text": "3. [MLOps] 機械学習開発運用環境の改善\n\n3.1. 概要\n\n社内で運用されている機械学習モデルの開発・運用ともに継続改善を行う\n\n\n\n3.2. 期間\n\n2024/11\n2025/03 - 現在\n\n\n\n3.3. 規模・役割\n\n役割：リーダー\n規模：私1名\n\n\n\n3.4. 担当業務\n\n機械学習関連の改善策立案・設計・実装\n\n\n\n3.5. 機能開発・実装詳細\n\nリポジトリへの CI/CD 導入\n\nLinter/Formatter を導入し Python/SQL/Terraform のコード品質を確保する\nStaging/Production へのデプロイを GitHub Actions の手動実行で行えるようにする\n\nデプロイ手順改善\n\nProduction 環境へのデプロイ実行はレビューを伴う形へと改善する\nリリースタグを用いた標準的なリリース管理を行う運用手順を整備する\n\nスコアリング結果のプロダクトへの連携システム構成の変更\n\nスコアリングをプロダクトアプリと同じ AWS アカウント内の ECS で実施していた構成から、より開発をしやすくスケーラビリティの確保が容易な Databricks でのスコアリング実施に変更する\n\n[検討中] 機械学習に利用するデータのデータマート化\n[検討中] プロダクトへのデータ反映確認の自動化\n[検討中] 学習データの増加に対応できるような処理改善\n[検討中] 機械学習モデルの精度を継続監視する仕組みの導入\n[検討中] テストの導入\n[検討中] モノレポ化\n\n\n\n3.6. 目的・背景\n\n取り扱う事業の増加・学習データの増加に対応していくために開発・運用を改善が求められている\n\n\n\n3.7. 課題\n\n数名のインターン生にロジック部分の開発に集中してもらうことで成果を出そうとしている中で、開発・運用に手作業が多く含まれることからコントリビューションの難易度が高くなってしまっている\n対象事業のアクティブユーザー数が増加することで学習データが増加しシステムのスケールが必要となる\n対象事業が増加する際の工数増加を懸念して新たな機械学習活用を提案しづらい状況にある\n\n\n\n3.8. 工夫した点\n\nまずできる所から改善を進めていき1年かけてゴールを目指すというマイルストーンを立て、恩恵を実感してもらいつつ協力を得ながら進める方針を取った点\n\n\n\n3.9. 成果\n\nデータサイエンスチームで最も注力しているオンラインクレーン事業の景品レコメンドシステムに関して CI/CD 導入・デプロイ手順改善を行いインターン生のコントリビューション難易度を下げることに成功しています。\n\n\n\n3.10. 担当フェーズ\n\n機械学習関連の改善策立案・設計・実装\n\n\n\n3.11. 開発環境\n\nGitHub\nGitHub Actions\nAWS\n\nECS\nRDS\nS3\n\nDatabricks\ndbt Core",
    "crumbs": [
      "職務経歴 詳細",
      "株式会社GENDA"
    ]
  },
  {
    "objectID": "details/05-genda.html#その他-開発環境等へのai導入推進",
    "href": "details/05-genda.html#その他-開発環境等へのai導入推進",
    "title": "株式会社GENDA",
    "section": "4. [その他] 開発環境等へのAI導入推進",
    "text": "4. [その他] 開発環境等へのAI導入推進\n\n4.1. 役割\n\nチーム内へのAI導入は私が主導して進めています。\n\n\n\n4.2. 所感\n\nデータエンジニアチームのコーディング作業では単調なカラム修正などが多いことから導入メリットが大きいです。\nデータサイエンスチームのコーディング作業ではコアとなる機械学習の実装に関して AI がまだあまり役に立たないことから一般的な Python コーディングの支援の観点で導入を進めています。\nデータ分析支援に MCP を活用できるか検討中です。感触は良いです。\n\n\n\n4.3. 各種ツールの検討状況 (2025年4月時点)\n\nGitHub Copilot\n\nコード補完は有用なので継続利用していきたい\n\nGitHub Copilot Chat\n\n後続の AI エージェントに比べて優位性がない\n\nDevin\n\n初回の環境構築・スタートアップスクリプトの作成を実施\nPR作成に限らない用途での利用例を見せてチーム内で更なる活用を促進\n\n執筆記事：Devin に週報を書いてもらった - Zenn\n\n\nAider\n\n任意の開発環境で利用できる CLI ベースの AI エージェントとして Aider の調査を実施\n\n執筆記事：Aider を使ってみる – uma-chan’s page\n\n任意の環境で AI エージェントを利用できるという観点で有用\n\nLLM の API 利用が必要で後述の Cursor を利用する場合2重課金になってしまうためあくまでも最終手段\n\n\nCursor\n\n社内で利用可能であったため検討を実施\nVS Code の fork であり VS Code の資源が活用できる点と既存コードを加味したコーディングが最もスムーズな点からローカル開発環境として最良の選択肢と考えています\nデータサイエンスチームでは Cursor + VS Code の Databricks 拡張の利用を検討しています\n\nGitHub Copilot Agent Mode (VS Code)\n\nCursor に比べて既存コードを考慮しない挙動になり Cursor の次点となる印象\n\nMCP\n\nDatabricks MCP サーバー\n\nLLM に実際に試行錯誤させながらデータ分析用の SQL を書かせることができる感触を得たのでビジネスサイドで SQL が書けない方にもデータ分析に参入してもらうための MCP 利用推進策を検討中です。\n\nCursor の活用\n\n公式ドキュメント・社内ドキュメント・他のリポジトリなどコンテキストの理解に各種サーバーを利用できるため Cursor を MCP クライアントに据えた開発環境改善方法を検討中です。",
    "crumbs": [
      "職務経歴 詳細",
      "株式会社GENDA"
    ]
  },
  {
    "objectID": "details/05-genda.html#その他-開発業務以外の活動",
    "href": "details/05-genda.html#その他-開発業務以外の活動",
    "title": "株式会社GENDA",
    "section": "5. [その他] 開発業務以外の活動",
    "text": "5. [その他] 開発業務以外の活動\n\n5.1. 記事執筆・掲載\n\n\n\n日付\n概要\nリンク\n\n\n\n\n2024/12 - 現在\nGENDA テックブログ執筆\numa-chanさんの記事一覧 - Zenn\n\n\n\n\n\n5.2. 採用活動\n\nカジュアル面談実施\n\nSNS にてカジュアル面談を実施している旨宣伝を行い面談に繋げました。\n\n転職媒体での候補者選定\n\n自主的に採用業務への関与を申し出て以降、現在まで継続的に候補者選定業務を担当しております。",
    "crumbs": [
      "職務経歴 詳細",
      "株式会社GENDA"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "職務経歴書",
    "section": "",
    "text": "現住所：北海道札幌市\n職務経歴書公開先：https://i9wa4.github.io/resume\nGitHub：https://github.com/i9wa4\nAsk DeepWiki about me：https://deepwiki.com/i9wa4/resume"
  },
  {
    "objectID": "index.html#職務要約",
    "href": "index.html#職務要約",
    "title": "職務経歴書",
    "section": "職務要約",
    "text": "職務要約\n北海道大学理学部数学科を卒業後、Windowsアプリエンジニアや組込エンジニアを6年間経験し、2022年以降はデータエンジニアとMLOpsエンジニアとして勤務しております。\n◆データエンジニアとしての経験： データ基盤の管理者・データ基盤の構築運用・データ基盤のIaC化・dbtによるデータマート構築運用・ダッシュボード作成運用のようにデータエンジニアリング全般に一気通貫で携わってきました。\n◆MLOpsエンジニアとしての経験： 二値分類による顧客の行動予測モデルの作成～ワークフロー構築運用保守・機械学習基盤のCI整備・機械学習基盤上でのプログラム実行エラー対応などに取り組んできました。\n◆チームの中での役割： シイエヌエス北海道にてプロジェクトのリーダーやサブリーダーを任されておりました。hacomonoでは一人データエンジニアとしてデータ基盤管理者・データ基盤や利活用に関するタスクの社内調整・データ基盤部の目標管理といったリーダー相当の業務を担当しておりました。現職GENDAではあるグループ企業のデータ利活用の窓口兼主担当として関係者との仕様調整やデータエンジニアリング全般を任されております。また業務進捗管理やレビューなどデータエンジニアチームのリーダー業務を2025年4月から徐々に受け持っております。\n◆採用活動への関与： チームを成長させることに対する興味関心が強く、採用活動に積極的に参加しております。新卒採用選考・面接官・カジュアル面談者の募集と面談実施・リファラル採用・転職媒体での候補者選定・ジョブディスクリプションの修正・求人の宣伝を行ってきました。"
  },
  {
    "objectID": "index.html#自己pr",
    "href": "index.html#自己pr",
    "title": "職務経歴書",
    "section": "自己PR",
    "text": "自己PR\n◆性格面： 周囲の社員に貢献できるような地道な業務にモチベーション高く取り組むことができます。具体的には CI/CD 整備・ドキュメント整備・勉強会開催・情報共有・採用活動などです。現在主に担当しているデータ利活用に向けた業務は、中長期的に周囲の社員に貢献できるものと信じて取り組めるため自分にマッチした業務であると感じています。"
  },
  {
    "objectID": "index.html#職務経歴",
    "href": "index.html#職務経歴",
    "title": "職務経歴書",
    "section": "職務経歴",
    "text": "職務経歴\n\n\n\n\n\n\n詳細は社名リンク先をご確認ください。\n\n\n\n\n正社員\n\n\n\n\n\n\nPeriod\n\n\n\nTitle\n\n\n\nDescription\n\n\n\n\n\n\n\n\n2024/11 - 現在\n\n\n株式会社GENDA\n\n\nデータエンジニア・MLOps エンジニアとして以下に従事 - データパイプライン全体の変更対応 - データマート整備 - ダッシュボード作成・運用 - AWS から Databricks への機械学習開発基盤移行対応 - CI 導入など機械学習基盤の整備 \n\n\n\n\n\n\n2024/04 - 2024/10\n\n\n株式会社hacomono\n\n\nデータエンジニアとして以下に従事 - 自社環境・顧客環境の DWH 構築・運用 - 自社データ基盤の運用改善・機能追加 \n\n\n\n\n\n\n2022/04 - 2024/03\n\n\n株式会社シイエヌエス北海道\n\n\nデータエンジニア・MLOps エンジニアとして以下に従事 - データパイプライン構築 - データマート整備 - 機械学習ワークフロー構築 \n\n\n\n\n\n\n2018/08 - 2022/03\n\n\n新光商事LSIデザインセンター株式会社\n\n\n組込エンジニアとして以下に従事 - 車載マイコンの機能開発 - インバータやモータの制御開発 \n\n\n\n\n\n\n2016/04 - 2018/07\n\n\nオークマ株式会社\n\n\nWindows アプリエンジニアとして以下に従事 - Windows 用 CAD/CAM アプリ開発の要件定義・テスト \n\n\n\n\n\n\nNo matching items\n\n\n\n業務委託\n\n\n\n\n\n\nPeriod\n\n\n\nTitle\n\n\n\nDescription\n\n\n\n\n\n\n\n\n2025/06 - 現在\n\n\nPIVOT株式会社\n\n\nデータエンジニア・MLOps エンジニア \n\n\n\n\n\n\nNo matching items\n\n\n\n最も苦労したプロジェクト\n現職の株式会社GENDAにて取り組んでいる以下です。 データ基盤の改修・データマート構築・ダッシュボード作成・全体の運用・データ分析支援といったデータに関すること全てを私が対応しており、プロジェクト進行上人員不足の面での課題に向き合い解消に向かっている最中です。\n[データ] カラオケ事業会社のデータ利活用推進 | 株式会社GENDA\n\n\n職務経歴 図解\n\n\n\n\n\n%%{\n    init: {\n        'logLevel': 'debug',\n        'theme': 'base',\n        'gitGraph': {\n            'rotateCommitLabel': true,\n            'showBranches': false\n        },\n        'themeVariables': {\n            'commitLabelFontSize': '14px',\n            'tagLabelFontSize': '12px'\n        }\n    }\n}%%\ngitGraph LR:\n    checkout main\n    commit id: \"北海道大学 理学部数学科 卒業\" tag: \"16.03\"\n\n    checkout main\n    branch b01 order: 1\n    commit id: \"オークマ株式会社 入社\" tag: \"16.04\" type: HIGHLIGHT\n    commit id: \"Windows アプリエンジニア\"\n    checkout main\n    merge b01 id: \"18.07\"\n\n    checkout main\n    branch b02 order: 2\n    commit id: \"新光商事LSIデザインセンター株式会社 入社\" tag: \"18.08\" type: HIGHLIGHT\n    commit id: \"組込エンジニア\"\n    checkout main\n    merge b02 id: \"22.03\"\n\n    checkout main\n    branch b03 order: 3\n    commit id: \"株式会社シイエヌエス北海道 入社\" tag: \"22.04\" type: HIGHLIGHT\n    commit id: \"データエンジニア / MLOps エンジニア-1\"\n    checkout main\n    merge b03 id: \"24.03\"\n\n    checkout main\n    branch b04 order: 4\n    commit id: \"株式会社hacomono 入社\" tag: \"24.04\" type: HIGHLIGHT\n    commit id: \"データエンジニア\"\n    checkout main\n    merge b04 id: \"24.10\"\n\n    checkout main\n    branch b05 order: 5\n    commit id: \"株式会社GENDA 入社\" tag: \"24.11\" type: HIGHLIGHT\n    commit id: \"データエンジニア / MLOps エンジニア-2\"\n\n    checkout main\n    commit id: \"25.05\"\n    branch b07 order: 7\n    commit id: \"PIVOT株式会社 業務委託として参画\" tag: \"25.06\" type: HIGHLIGHT\n    commit id: \"データエンジニア / MLOps エンジニア-3\"\n\n    %% --- Current Activity ---\n\n    %% checkout main\n    %% commit id: \"個人活動\"\n\n    checkout b05\n    commit id: \"就業中\" type: HIGHLIGHT\n\n    checkout b07\n    commit id: \"参画中\" type: HIGHLIGHT"
  },
  {
    "objectID": "index.html#今後取り組みたいこと",
    "href": "index.html#今後取り組みたいこと",
    "title": "職務経歴書",
    "section": "今後取り組みたいこと",
    "text": "今後取り組みたいこと\n◆役割面： 直近：リーダー経験・マネジメント経験を積む 3-5年後：データ組織でリーダーシップを発揮し横断的にコミュニケーションを取りながらビジネス上の課題解決を目指す\n◆技術面： データ基盤管理者として各種サービスの知見を深める データ活用に繋がる分野 (BIツール・データカタログ・データマート作成など) へ積極関与する 特徴量作成・データマート改善のようにデータの観点から機械学習に貢献する AIを活用して開発環境改善やデータ分析の簡易化 (民主化) に貢献する"
  },
  {
    "objectID": "index.html#スキルレベル",
    "href": "index.html#スキルレベル",
    "title": "職務経歴書",
    "section": "スキルレベル",
    "text": "スキルレベル\n★：業務経験あり\n\n\n\n\n\n\n\n\n\n項目\n種類\n使用期間\nレベル\n\n\n\n\nIaC\nDocker\n ★3年\nDockerfile や Docker Compose の記述・実行が可能\n\n\nIaC\nGitHub / CodeCommit\n ★4年\nGit flow や GitHub flow に則った開発利用やレビューが可能\n\n\nIaC\nGitHub Actions\n ★1年\n既存ワークフロー改修 / CI/CD 向けのワークフロー作成 / GitHub Pages 関連のワークフロー作成が可能\n\n\nIaC\nTerraform\n ★1年\nAWS / Google Cloud (特に BigQuery) / Snowflake を用いた基盤の修正作業や IaC 化が可能\n\n\nData\nBigQuery\n ★0.5年\n管理者業務や IaC 化作業が可能\n\n\nData\nEmbulk\n ★0.5年\n転送元 DB や 利用するインスタンス性能に応じたパフォーマンス改善が可能\n\n\nData\nTROCCO\n ★0.5年\nジョブ作成・運用が可能\n\n\nData\ndbt Cloud\n ★0.5年\nジョブ作成・運用が可能\n\n\nData\ndbt Core\n ★1年\nプロジェクト新規作成から運用まで可能\n\n\nData / ML\nDatabricks\n ★0.5年\n管理者業務や dbt Core との連携によるデータマート構築・ダッシュボード作成・機械学習環境整備が可能\n\n\nData / ML\nSnowflake\n ★2年\nデータマート作成 / SQL パフォーマンス改善 / Snowpark ML 利用 / パラメータ管理が可能\n\n\nML\nDataRobot\n ★2年\nオートパイロットや blueprint 固定でのモデルデプロイ・運用、API を利用した各種評価指標の取得の実装が可能\n\n\nOS\nAmazon Linux / Ubuntu\n ★4年\n要件に応じた環境構築や開発環境利用が可能\n\n\nOS\nWindows / macOS\n ★9年\n開発利用が可能\n\n\n言語\nC\n ★3年\n一人称で作業可能\n\n\n言語\nPython\n ★4年\n一人称で作業可能\n\n\n言語\nSQL\n ★3年\n一人称で作業可能\n\n\n言語\nShell Script\n ★4年\n一人称で作業可能\n\n\nCloud\nAWS\n ★3年\nEC2 / ECS / RDS / StepFunctions / SageMaker / Cloud9 / MWAA (Airflow) 等を含む環境構築や作業が可能\n\n\nCloud\nGoogle Cloud\n ★0.5年\nBigQuery 中心としたプロジェクトの管理者業務や Compute Engine / Cloud Functions / Datastream / BigQuery (後述) 等を含む環境構築や作業が可能\n\n\nDB\nMySQL\n ★1年\nパフォーマンス調整のために必要なパラメータ調整の指示やデータ閲覧が可能\n\n\nDB\nPostgreSQL\n ★0.5年\nローカル環境でのデータベース作成・データ閲覧が可能"
  },
  {
    "objectID": "index.html#資格",
    "href": "index.html#資格",
    "title": "職務経歴書",
    "section": "資格",
    "text": "資格\n\n[2023/04] 統計検定2級\n[2015/03] TOEIC スコア 805"
  },
  {
    "objectID": "index.html#通常業務以外の活動",
    "href": "index.html#通常業務以外の活動",
    "title": "職務経歴書",
    "section": "通常業務以外の活動",
    "text": "通常業務以外の活動\n\n記事執筆・掲載\n\n\n\n日付\n概要\nリンク\n\n\n\n\n2024/12 - 現在\nGENDA テックブログ執筆\numa-chanさんの記事一覧 - Zenn\n\n\n2024/10/08\nFindy Tools 様 特集記事掲載\n39社のデータ基盤アーキテクチャ特集 - ツールの技術選定のポイントと活用術 - Findy Tools\n\n\n2024/08/21\nFindy Tools 様 レビュー執筆\n株式会社hacomonoのBigQuery導入事例 - Findy Tools\n\n\n2024/07/02\nhacomono テックブログ執筆\nhacomonoデータ基盤におけるデータ転送の課題と今後の対応 - hacomono TECH BLOG"
  },
  {
    "objectID": "details/03-cnsh.html",
    "href": "details/03-cnsh.html",
    "title": "株式会社シイエヌエス北海道",
    "section": "",
    "text": "Table 1: 会社概要・雇用形態 (2024年時点)\n\n\n\n\n\n\n\n\n\n事業内容\nアプリケーション開発・インフラ構築事業、クラウド構築事業、データ分析・AI事業、DX支援事業\n\n\n資本金\n2500万円\n\n\n売上高\n7億5,351万円\n\n\n従業員数\n39名\n\n\n上場／非上場\n非上場\n\n\n勤務地\n北海道札幌市週に約4回リモートワーク実施\n\n\n雇用形態\n正社員\n\n\n所属\n[2022/04 - 2024/03] デジタルビジネス推進部",
    "crumbs": [
      "職務経歴 詳細",
      "株式会社シイエヌエス北海道"
    ]
  },
  {
    "objectID": "details/03-cnsh.html#mlops-情報通信企業向け-機械学習ワークフローと-ai-プラットフォーム移管",
    "href": "details/03-cnsh.html#mlops-情報通信企業向け-機械学習ワークフローと-ai-プラットフォーム移管",
    "title": "株式会社シイエヌエス北海道",
    "section": "1. [MLOps] 情報通信企業向け 機械学習ワークフローと AI プラットフォーム移管",
    "text": "1. [MLOps] 情報通信企業向け 機械学習ワークフローと AI プラットフォーム移管\n\n1.1. 概要\n\nDataRobot 中心で構築していた機械学習ワークフローから Snowpark ML (Snowflake) 中心の構築への移管\n\n\n\n1.2. 期間\n\n[2023/12 - 2024/01] Snowpark ML 事前検証\n[2024/02 - 2024/03] 追加調査・ワークフロー設計\n\n\n\n1.3. 規模・役割\n\n[2023/12 - 2024/01]\n\n役割：メンバー\n規模：1名 (私)\n\n[2024/02 - 2024/03]\n\n役割：リーダー\n規模：2名\n\n自社の若手2名\n\n\n\n\n\n1.4. 担当業務\n\n[2023/12 - 2024/01] (私1名) Snowpark ML の事前検証\n[2024/02 - 2024/03] (2名) DataRobot から Snowpark ML への移管のための追加調査\n[2024/03] (私1名) ワークフロー設計\n\n\n\n1.5. 機能開発・実装詳細\n移管前後の構築詳細は以下の通りです。\n\n移管前構築 (AWS MWAA + ECS + DataRobot + Snowflake)\n\nMWAA にてワークフロー管理を行っており DAG ファイルにてジョブを定義し並列処理や直列処理を記述していました。\nMWAA KubernetesPodOperator にて利用する ECS イメージを JupyterHub (ノートブック実行環境) でも共有していたため本番と開発でライブラリ依存関係の齟齬がない環境でした。\nAI プラットフォームとして DataRobot を採用し、ハイパーパラメータチューニングやモデル評価を全てフルマネージドで実施させていました。ただしコストが嵩む問題がありました。\n\n移管後構築 (AWS SageMaker + Snowpark ML + Snowflake)\n\nAI プラットフォームとして Snowpark ML を採用することとしました。使用感は scikit-learn に近く、ラベルエンコーディング・ハイパーパラメータチューニング・モデル評価を自力で実装する必要があります。\nワークフローは “SageMaker + Papermill” でノートブック上での管理となります。\nモデルはモデルレジストリ機能にて管理させます。\n他部署 (他の環境) とも共有できるように素朴な実装を目指しました。\n\n\n\n\n1.6. 目的・背景\n\nコスト上の問題から DataRobot の利用を2024年8月に停止するという判断を受けて AI プラットフォームの移行先を探す必要がありました。\n\n\n\n1.7. 課題\n\n移行業務は付加価値を生みづらい都合で移行先調査の優先順位が低くなってしまい、分析基盤管理側の社員も移行先調査がほとんど実施できていない状況でした。\nSnowpark ML は新しいサービスのため公式ドキュメント以外で参照出来る情報が社内外ともにかなり限られている状況でした。\n\n\n\n1.8. 工夫した点\n\n調査作業での工夫点\n\n後から続く方が参照しやすいように調査記録を丁寧に取りました。\n\nSnowflake 公式のリポジトリに置かれているサンプルコードや公式ドキュメント・公式ライブラリソースコードのどの部分を参照したか記録に残しました。\n調査記録を客先社内公開済み。\n\n必要最低限の領域までの調査に留めることで、スケジュールや実装方針について予定より早く合意を取ることができました。\n\n関係者が多くなるため情報提供をいち早く行うことを優先しました。\n\n\n\n\n\n1.9. 成果\n\nSnowpark ML の利用は前例が少なかったため Snowflake 社からの個別開示情報などを利用して情報収集と事前検証を行ないました。結果、既存ワークフローに比べてコスト・性能面・セキュリティ面で優位性があることを示し、客先での先行事例を作ることができました。\n\n価値を生みづらいと見込まれていた移管作業の意義を示したため、注目度を上げ、周囲を巻き込むことに成功しました。客先での社内政治的にも Snowpark ML 導入推進の話を通しやすくなる状況作りに貢献しました。\n\n本プロジェクトではリーダーとして技術調査・スケジュール策定・チームへの作業指示・顧客他部署への情報共有など多様な役割を担うことができました。\n\n\n\n1.10. 担当フェーズ\n\n要件分析\n基本設計\n詳細設計\n開発\nテスト\n\n\n\n1.11. 開発環境\n\nAWS\n\nSageMaker\nCodeCommit\nSecrets Manager\n\nSQL\n\nSnowflake\n\nPython 実行環境\n\nSageMaker (conda_python3)\n\nAI プラットフォーム\n\nSnowpark ML",
    "crumbs": [
      "職務経歴 詳細",
      "株式会社シイエヌエス北海道"
    ]
  },
  {
    "objectID": "details/03-cnsh.html#データ-情報通信企業向け-データマート整備",
    "href": "details/03-cnsh.html#データ-情報通信企業向け-データマート整備",
    "title": "株式会社シイエヌエス北海道",
    "section": "2. [データ] 情報通信企業向け データマート整備",
    "text": "2. [データ] 情報通信企業向け データマート整備\n\n2.1. 概要\n顧客情報と紐付けて分析や機械学習に利用するための下記特徴量の作成\n\nポイントサービス加盟店と顧客推定住所との距離\nポイントサービス詳細情報\n決済情報\n様々な特徴量を次元圧縮して生成する特徴量\n行動履歴からルールベースで推定されたライブイベント実績\n\n\n\n2.2. 期間\n\n[2023/04 - 2023/05] ポイントサービス加盟店と顧客推定住所との距離\n[2023/06 - 2023/07] 決済情報\n[2023/08 - 2023/09] 行動履歴からルールベースで推定されたライフイベント実績\n[2023/10 - 2023/12] 様々な特徴量を次元圧縮して生成する特徴量\n[2024/01 - 2024/03] ポイントサービス詳細情報\n\n\n\n2.3. 規模・役割\n\n役割：サブリーダー\n規模：4名\n\n\n\n2.4. 担当業務\n\n(主に私1名) データ作成月次作業向けワークフロー構築\n(4名) データレイク調査\n(4名) データ抽出クエリ作成\n(リーダーと私の2名) レビュー\n(4名) 検証\n(4名) 保守\n(4名) 月次運用作業\n\n\n\n2.5. 機能開発・実装詳細\nデータマート構成のためのクエリは Juupyter Notebook 上から Python API 経由で実行していましたが、2023年秋頃から dbt Core を導入し SQL ファイルベースへ置き替えつつある状況でした。\n\n[旧環境] Snowflake の Python API 利用 (Jupyter Notebook にて管理)\n\nSnowflake Python API を Jupyter Notebook から実行します。\n\nファイル数が増えたりコード量が増えるとコードの見通しが悪くなりやすい欠点があります。\n\nワークフローは MWAA にて構築しています。\n\n[新環境] dbt Core 利用 (SQL ファイルベース)\n\ndbt Core により SQL ファイル主体で管理を行います。\n\n入出力の依存関係が明示され管理が容易になります。\nCI/CD、自動テスト、自動ドキュメント生成など現代的な開発・運用が可能となります。\n必要に応じて増分データのみを対象とするクエリに変更できます。\n\n\n\n\n\n2.6. 目的・背景\n\nデータレイクは充実しているものの、機械学習や顧客情報分析に利用できるデータマートは発展途上のため適宜追加していく必要がありました。\n\n\n\n2.7. 課題\n\n分析に利用できる特徴量が不足している状況でした。\n従来作成していた特徴量が上流データの仕様変更や不具合によって作ることができない場合がありました。\nJupyter Notebook で管理しているデータマート向けプログラムが長大なため不具合発生時の原因究明に時間が掛かることが多くありました。\n上流データが全て揃ったタイミングでデータマートの月次更新を行っておりましたが、一部情報をできるだけ早く更新してほしいと言われるようになりました。\n\n\n\n2.8. 工夫した点\n\n\n分析に利用できる特徴量が不足している状況でした。\n従来作成していた特徴量が上流データの仕様変更や不具合によって作ることができない場合がありました。\n\n\n\nユーザーアンケートにより整備すべきデータを決定し、より必要とされるデータマートを目指しました。\nできるだけ早くデータを提供できるような上流データを選定しつつ、上流データの障害発生率を調査し取捨選択を行いました。信頼性の高いデータマート運用のために設計時に十分考慮を行いました。\n\n\n\nJupyter Notebook で管理しているデータマート向けプログラムが長大なため不具合発生時の原因究明に時間が掛かることが多くありました。\n上流データが全て揃ったタイミングでデータマートの月次更新を行っておりましたが、一部情報をできるだけ早く更新してほしいと言われるようになりました。\n\n\n\nいずれも解消できるポテンシャルをもつ dbt に移行することとしました。\n\n\n\n2.9. 成果\n\nユーザー目線での改善を続けたことでデータマートのアクセス数増加につながり、部内の大きな成果として表彰を受けました。\n自チーム・他チームともに顧客行動予測を行っており、そのために必要な特徴量を自力で作成し社内の多様なモデルの精度向上に貢献できました。\ndbt 移行を推進することでソースコードを SELECT 文中心の理解しやすいものに置き換えることができました。\n上流データが追加されたタイミングで必要に応じてデータ作成処理を行える (セマンティックレイヤ対応ができる) ように dbt 環境に移行を順次進めることができています。\n\nセマンティックレイヤ対応は dbt Core では非対応のため dbt Cloud 導入時の対応となります。\n\n\n\n\n2.10. 担当フェーズ\n\n要件分析\n基本設計\n詳細設計\n開発\nテスト\n運用\n保守\n\n\n\n2.11. 開発環境\n\nAWS\n\nCodeCommit\nCloud9\nSecrets Manager\nSageMaker\nMWAA\nEKS\nECS\n\nSQL\n\nSnowflake\ndbt\n\nPython 実行環境\n\nSageMaker (conda_python3)\nJupyterHub (conda_python3)",
    "crumbs": [
      "職務経歴 詳細",
      "株式会社シイエヌエス北海道"
    ]
  },
  {
    "objectID": "details/03-cnsh.html#mlops-情報通信企業向け-機械学習ワークフローのクラウドシフト",
    "href": "details/03-cnsh.html#mlops-情報通信企業向け-機械学習ワークフローのクラウドシフト",
    "title": "株式会社シイエヌエス北海道",
    "section": "3. [MLOps] 情報通信企業向け 機械学習ワークフローのクラウドシフト",
    "text": "3. [MLOps] 情報通信企業向け 機械学習ワークフローのクラウドシフト\n\n3.1. 概要\n\nオンプレ基盤で実施していた顧客行動予測モデル作成 (50モデル分) ワークフローのクラウドシフト\n\n\n\n3.2. 期間\n\n[2022/07 - 2022/10] 28モデル分\n[2022/11 - 2023/03] 22モデル分\n\n\n\n3.3. 規模・役割\n\n規模：4名\n役割：メンバー\n\n\n\n3.4. 担当業務\n\n(私1名) ワークフロー設計・実装\n(私1名) 各ジョブ設計\n(4名) 各ジョブ実装\n(4名) 上流データ調査\n(4名) 機械学習用データ作成クエリの再構成\n(4名) 検証\n(4名) 保守\n\n\n\n3.5. 機能開発・実装詳細\nMWAA (Airflow) + EKS + Kubernetes + Papermill な Python 実行環境が構築済みのため以下の要領で実装を行いました。\n\nMWAA ワークフローを設計し DAG ファイルとして実装\n\n大まかにはモデル共通処理を実行した後、モデル固有の処理を並列で実行する流れです。\n\nSnowflake Python API と DataRobot API を実行する Python コード (Jupyter Notebook) を各ジョブの設計内容に応じて作成\n\n機械学習用データ作成・モデリング・スコアリング・モデル評価指標取得などの処理を実装します。\n\n\n\n\n3.6. 目的・背景\n\n従来の基盤の廃止に伴い AWS 基盤にて機械学習ワークフローを構築する必要がありました。\n\n\n\n3.7. 課題\n\n上流データは仕様が変わりつつ移行済みだったため、従来基盤で利用していたクエリと同等のものを作るには調査・検証の時間を大きく取る必要がありました。\nMWAA と DataRobot に関するノウハウがチーム内になく、自力で調査をしつつ基盤担当者から情報提供を受けながら取り組む必要がありました。\n\n\n\n3.8. 工夫した点\n\nMWAA (Airflow) + EKS + Kubernetes + Papermill の構成で並列処理に強いことを利用し、並列処理を最大限活用できるようにジョブの粒度を調整しました。\n移行対象のクエリの量が膨大かつ移行先上流データがどれか分からない状況だったため、初期調査段階では件数比較を活用しある程度当たりを付ける方法で効率的に設計を進めました。\n\n最終的にはレコードの一致率確認を行っていますが、初めから一致率確認をしていると時間が足りなかったと思われます。\n\nMWAA による機械学習ワークフロー構築時に従来手動で実施していた検証処理も含め、最終的に Slack 通知確認で完了するように作業を簡素化することで単純な移行ではなく価値を生むことを意識しました。\n\n\n\n3.9. 成果\n\n従来と同等のワークフローを維持しつつ、自動化と時短の工夫を入れることで大幅に運用工数を減らすことができました。\n\n\n\n3.10. 担当フェーズ\n\n要件分析\n基本設計\n詳細設計\n開発\nテスト\n保守\n運用\n\n\n\n3.11. 開発環境\n\nAWS\n\nSageMaker\nCodeCommit\nSecrets Manager\nMWAA\nEKS\nECS\n\nSQL\n\nSnowflake\n\nPython 実行環境\n\nSageMaker (conda_python3)\nJupyterHub (conda_python3)\n\nAI プラットフォーム\n\nDataRobot",
    "crumbs": [
      "職務経歴 詳細",
      "株式会社シイエヌエス北海道"
    ]
  },
  {
    "objectID": "details/03-cnsh.html#mlops-小売企業向け-客数予測処理実行環境構築",
    "href": "details/03-cnsh.html#mlops-小売企業向け-客数予測処理実行環境構築",
    "title": "株式会社シイエヌエス北海道",
    "section": "4. [MLOps] 小売企業向け 客数予測処理実行環境構築",
    "text": "4. [MLOps] 小売企業向け 客数予測処理実行環境構築\n\n4.1. 概要\n\nオンプレ基盤で実施していた客数予測処理の AWS 環境移行\n\n\n\n4.2. 期間\n\n2022/05 - 2022/06\n\n\n\n4.3. 規模・役割\n\n1名\nメンバー\n\n\n\n4.4. 担当業務\n\n予測処理実行環境構築 (EC2, Python)\n環境変更に伴う Python コード修正・スクリプト作成\n予測処理実行手順構築\n\n\n\n4.5. 機能開発・実装詳細\n\nEC2 に Anaconda をインストールしベイズ統計モデル作成環境を構築します。\n顧客の要求により客数予測に費せる日数が4日程度のためその範囲内で十分終了するよう処理の並列化を行います。\n\n\n\n4.6. 目的・背景\n\n(前提) PoC として1店舗毎にベイズ統計モデルで客数予測を行うプログラムが作成されており、客数予測処理は1店舗あたり4時間程度かかる状況でした。\n\nその上で4日程度で約200店舗分の客数予測を出す環境構築を求められていました。\nインスタンス性能を上げるための AWS 移行となります。\n\n\n\n\n4.7. 課題\n\n要求を満たすためには EC2 高性能インスタンスを利用する必要がありコストは最小限とする必要がありました。\n前任者の作成した並列処理に問題があり総実行時間が長くなってしまいまう。\n経験の浅い作業者に運用を直ちに引き継ぐことになっていたため、手動実行するプロセスを挟むと不具合や遅延が想定されました。\n\n\n\n4.8. 工夫した点\n\n前任者の作成していた並列処理は先にコア数に基づきジョブを分割する形式のシェルスクリプトで構成していましたが、各店舗毎にデータ量が異なるため予測処理が早めに終了して遊びの出るコアが発生してしまっていました。\n\n改善のため xargs に並列処理を管理させることで空きコアに逐次ジョブを実行させる構成に改善させ総実行時間を短縮させることができました。\n\n\n\n\n4.9. 成果\n\n当初の構成に比べて総実行時間を短縮することができ EC2 インスタンス利用時間の削減に成功しました。\n総実行時間は2日程度に抑えることができ、顧客要求を満たすことができました。\n利用可能なリソースが EC2 のみという制約の中、スクリプトを活用し極力自動化し引継ぎ後の業務負荷を軽減させました。\n\n\n\n4.10. 担当フェーズ\n\n要件分析\n基本設計\n詳細設計\n開発\nテスト\n保守\n\n\n\n4.11. 開発環境\n\nAWS\n\nEC2\n\nSQL\n\nPostgreSQL\n\nGit\nPython 実行環境\n\nPython (公式)\nAnaconda\n\nシェルスクリプト",
    "crumbs": [
      "職務経歴 詳細",
      "株式会社シイエヌエス北海道"
    ]
  },
  {
    "objectID": "details/03-cnsh.html#その他-開発業務以外の活動",
    "href": "details/03-cnsh.html#その他-開発業務以外の活動",
    "title": "株式会社シイエヌエス北海道",
    "section": "5. [その他] 開発業務以外の活動",
    "text": "5. [その他] 開発業務以外の活動\n\n5.1. 新卒採用一次選考の面接官対応\n\n2024年新卒採用：学生6名分\n2025年新卒採用：学生6名分\n\n\n\n5.2. データ分析業務担当者育成\n\n育成用プログラム進捗管理 (2023/06 - 2023/12)\n\n利用教材：データサイエンス100本ノック\n育成対象者：7名 (2023年度)\n業務：環境構築手順作成・質疑応答対応・作業進捗管理 (すべて私1名で対応)\n\nローカル上の環境構築手順の共有 (随時)\n\n社内 Wiki にて記事作成\n具体例\n\nAmazon Linux (EC2) + Anaconda + PyStan 環境構築\nWindows + Anaconda + PyStan 環境構築\nUbuntu (WSL2) + Docker (Dockerfile) + CPython + venv 環境構築\nUbuntu (WSL2) + Docker 環境構築 (データサイエンス100本ノック向け)\nAmazon Linux (Cloud9) + pyenv 環境構築\nVSCode と Vim での Linter Formatter 導入方法\n\n\nデータ分析関連技術情報共有 (随時)\n\nニュースサイト・技術ブログ・技術コミュニティにて情報収集し、社内チャットにて随時共有",
    "crumbs": [
      "職務経歴 詳細",
      "株式会社シイエヌエス北海道"
    ]
  },
  {
    "objectID": "details/04-hacomono.html",
    "href": "details/04-hacomono.html",
    "title": "株式会社hacomono",
    "section": "",
    "text": "Table 1: 会社概要・雇用形態 (2024年時点)\n\n\n\n\n\n\n\n\n\n事業内容\n月額型店舗のための会員管理・予約・キャッシュレス決済システム「hacomono」開発・販売\n\n\n資本金\n100百万円\n\n\n売上高\n非公開\n\n\n従業員数\n217名\n\n\n上場／非上場\n非上場\n\n\n雇用形態\n正社員\n\n\n勤務地\nフルリモート (居住地 北海道札幌市)年に数回東京原宿本社へ出社\n\n\n所属\n[2024/04 - 2024/08] 基盤本部 データ基盤部[2024/09 - 2024/10] データ本部 データ基盤部 (組織改編)",
    "crumbs": [
      "職務経歴 詳細",
      "株式会社hacomono"
    ]
  },
  {
    "objectID": "details/04-hacomono.html#データ-顧客環境-bigquery-へのバッチ転送システム開発運用",
    "href": "details/04-hacomono.html#データ-顧客環境-bigquery-へのバッチ転送システム開発運用",
    "title": "株式会社hacomono",
    "section": "1. [データ] 顧客環境 BigQuery へのバッチ転送システム開発・運用",
    "text": "1. [データ] 顧客環境 BigQuery へのバッチ転送システム開発・運用\n\n1.1. 概要\n\nhacomono アプリDBに蓄積されたデータを顧客環境 BigQuery へ日次バッチ転送するためのシステム開発・運用\n\n\n\n1.2. 期間\n\n2024/04 - 2024/10\n\n\n\n1.3. 規模・役割\n\n[2024/04 - 2024/05]\n\n役割：メンバー\n規模：2名\n\n[2024/06 - 2024/10]\n\n役割：リーダー\n規模：1名\n\n\n\n\n1.4. 担当業務\n\n[2024/04 - 2024/05] (2名) 定常運用業務・業務引き継ぎ\n[2024/06 - 2024/10] (私1名) 定常運用業務・障害対応\n\n\n\n1.5. 機能開発・実装詳細\n\nGitHub と Terraform (Terragrunt) を用いて以下の構成を実装します。\n\nRDS MySQL or Aurora MySQL をデータソースとして Embulk で顧客環境 BigQuery にデータ転送を行う\n\nシステム構成図は下記ブログをご参照ください。\n\nhacomonoデータ基盤におけるデータ転送の課題と今後の対応 - hacomono TECH BLOG\n\n\n\n\n1.6. 目的・背景\n\n顧客に向けて DWH 提供を行うサービスを開始するために暫定的に構築したシステムを運用し続けている状況です。\n\n\n\n1.7. 課題\n\n全量のデータを転送させているためデータ増加により転送時間が延びてしまい、具体的には以下の問題が起こるようになりました。\n\nRDS BurstBalance を使い切ることによる転送の不安定化が起こる\nデータソース側の DB で一時領域不足になる\n顧客約束の時刻に転送が間に合わなくなる\n\n\n\n\n1.8. 工夫した点\n\n問題発生源の特定のために Embulk のログやデータソースのメトリクスから真因を究明するよう努めました。\n\n\n\n1.9. 成果\n\n\nRDS BurstBalance を使い切ることによる転送の不安定化が起こる\n\n\nReadIOPS の数値が支配的だったため並列実行数を減らしたり、Embulk の読み込み量のパラメータを調整したりすることで BurstBalance を使い切らないまま可能な限り早く転送させるよう調整を行いました。\n\n\n1.10. 担当フェーズ\n\n運用\n保守\n\n\n\n1.11. 開発環境\n\nGitHub\nTerraform\nEmbulk\nAWS\n\nECS\nRDS MySQL\nAurora MySQL\nVPC\n\nGoogle Cloud\n\nBigQuery",
    "crumbs": [
      "職務経歴 詳細",
      "株式会社hacomono"
    ]
  },
  {
    "objectID": "details/04-hacomono.html#データ-社内-bigquery-環境構築運用",
    "href": "details/04-hacomono.html#データ-社内-bigquery-環境構築運用",
    "title": "株式会社hacomono",
    "section": "2. [データ] 社内 BigQuery 環境構築・運用",
    "text": "2. [データ] 社内 BigQuery 環境構築・運用\n\n2.1. 概要\n\nhacomono プロダクトのデータベースから BigQuery へデータを転送し社内向けに DWH を作成するシステムの運用・保守\n\n\n\n2.2. 期間\n\n2024/04 - 2024/10\n\n\n\n2.3. 規模・役割\n\n[2024/04 - 2024/05]\n\n役割：メンバー\n規模：2名\n\n[2024/06 - 2024/10]\n\n役割：リーダー\n規模：1名\n\n\n\n\n2.4. 担当業務\n\n[2024/04 - 2024/05] (2名) 定常運用業務・業務引き継ぎ\n[2024/06 - 2024/10] (私1名) 定常運用業務・運用改善業務\n\n\n\n2.5. 機能開発・実装詳細\n\n(新規実装) Google Cloud プロジェクトと BigQuery の権限管理機能\n\n\n\n2.6. 目的・背景\n\n社内の分析需要に応えるために権限管理の行き届いた新しい BigQuery 環境を構築・運用が必要となりました。\n\n\n\n2.7. 課題\n\n\n(新規実装) Google Cloud プロジェクトと BigQuery の権限管理機能\n\n\n概ね IaC 対応が済んでいましたが権限追加に関してはコンソールからの手動追加対応となっておりました。これにより環境差分が生じ terraform apply が通らなくなる状況でした。\n\n\n2.8. 工夫した点\n\n\n(新規実装) Google Cloud プロジェクトと BigQuery の権限管理機能\n\n\n権限を最小限に留めるためにプロジェクトレベルでの権限付与をせず、データセット毎に権限を付与する運用に変更しました。\n\n\n2.9. 成果\n\n\n(新規実装) Google Cloud プロジェクトと BigQuery の権限管理機能\n\n\nIaC 化により権限の種類とユーザーを CSV ファイルに集約でき、権限付与状況をリポジトリで管理できるようになりました。\n\n\n2.10. 担当フェーズ\n\n運用\n保守\n\n\n\n2.11. 開発環境\n\nGitHub\nTerraform\nEmbulk\nAWS\n\nECS\nRDS MySQL\nAurora MySQL\nVPC\n\nGoogle Cloud\n\nBigQuery",
    "crumbs": [
      "職務経歴 詳細",
      "株式会社hacomono"
    ]
  },
  {
    "objectID": "details/04-hacomono.html#データ-顧客環境-bigquery-へのリアルタイム転送システム開発運用",
    "href": "details/04-hacomono.html#データ-顧客環境-bigquery-へのリアルタイム転送システム開発運用",
    "title": "株式会社hacomono",
    "section": "3. [データ] 顧客環境 BigQuery へのリアルタイム転送システム開発・運用",
    "text": "3. [データ] 顧客環境 BigQuery へのリアルタイム転送システム開発・運用\n\n3.1. 概要\n\nhacomono に蓄積されたデータを顧客環境 BigQuery へリアルタイム転送するためのシステム開発・運用\n2024年8月まで開発を続けていましたが、契約締結できず検証段階で開発を中断しています。\n\n\n\n3.2. 期間\n\n2024/04 - 2024/08\n\n\n\n3.3. 規模・役割\n\n[2024/04 - 2024/05]\n\n役割：メンバー\n規模：2名\n\n[2024/06 - 2024/08]\n\n役割：リーダー\n規模：2名\n\n\n\n\n3.4. 担当業務\n\n[2024/04 - 2024/05] (2名) 開発引き継ぎ\n[2024/06 - 2024/08] 主機能開発・進捗管理\n\n\n\n3.5. 機能開発・実装詳細\n\nGitHub と Terraform (Terragrunt) を用いて以下の構成を実装します。\n\n[主機能] RDS MySQL のデータを Datastream によってリアルタイムに転送させる\n\n(新規実装) [周辺機能] ログ出力\n\n\n\n\n\n3.6. 目的・背景\n\n顧客要望がありリアルタイム性を重視したテーブルに絞って機能を提供することとなりました。\n個社向け開発として進めていますが他社への展開も可能な状態に整備する必要があります。\n\n\n\n3.7. 課題\n\n前任者から引き継いだ状態のリポジトリが雑然としており各社向けの設定ファイルを切り替えることができず大規模なリファクタリングが必要でした。\nDatastream の知見のある方が社内にいないため障害発生時の対応のノウハウがない状態でした。\n\n\n\n3.8. 成果\n\n\nDatastream の知見のある方が社内にいないため障害発生時の対応のノウハウがない状態でした。\n\n\nRDS 再起動による接続断から復旧する必要がありストリームの復元を行うことがありましたが、この経験により障害発生時に欠損し得るデータに関して説明ができるようになりました。\n本機能のオプションサービス化にあたり懸念事項の解像度を上げ、サービスのマニュアル更新の準備に役立てることができました。\n\n\n3.9. 担当フェーズ\n\n詳細設計\n開発\nテスト\n運用\n保守\n\n\n\n3.10. 開発環境\n\nGitHub\nTerraform\nTerragrunt\nAWS\n\nRDS MySQL\nVPC\n\nGoogle Cloud\n\nBigQuery\nDatastream",
    "crumbs": [
      "職務経歴 詳細",
      "株式会社hacomono"
    ]
  },
  {
    "objectID": "details/04-hacomono.html#その他-開発業務以外の活動",
    "href": "details/04-hacomono.html#その他-開発業務以外の活動",
    "title": "株式会社hacomono",
    "section": "4. [その他] 開発業務以外の活動",
    "text": "4. [その他] 開発業務以外の活動\n\n4.1. 記事執筆・掲載\n\n\n\n日付\n概要\nリンク\n\n\n\n\n2024/10/08\nFindy Tools 様 特集記事掲載\n39社のデータ基盤アーキテクチャ特集 - ツールの技術選定のポイントと活用術 - Findy Tools\n\n\n2024/08/21\nFindy Tools 様 レビュー執筆\n株式会社hacomonoのBigQuery導入事例 - Findy Tools\n\n\n2024/07/02\nhacomono テックブログ執筆\nhacomonoデータ基盤におけるデータ転送の課題と今後の対応 - hacomono TECH BLOG",
    "crumbs": [
      "職務経歴 詳細",
      "株式会社hacomono"
    ]
  },
  {
    "objectID": "details/01-okuma.html",
    "href": "details/01-okuma.html",
    "title": "オークマ株式会社",
    "section": "",
    "text": "Table 1: 会社概要・雇用形態 (2018年時点)\n\n\n\n\n\n\n\n\n\n事業内容\nCNC工作機械、CNC装置・サーボモータ・位置検出器・ソフトウェアの開発・製造、販売\n\n\n資本金\n180億円\n\n\n売上高\n1600億円\n\n\n従業員数\n3407名\n\n\n上場／非上場\n上場\n\n\n勤務地\n愛知県\n\n\n雇用形態\n正社員\n\n\n所属\n[2016/04 - 2017/03] 人事部付[2017/04 - 2018/07] FAシステム本部 FA開発部 ITプラザ開発課",
    "crumbs": [
      "職務経歴 詳細",
      "オークマ株式会社"
    ]
  },
  {
    "objectID": "details/01-okuma.html#windows7-向け対話形式-3d-cadcam-アプリケーション開発",
    "href": "details/01-okuma.html#windows7-向け対話形式-3d-cadcam-アプリケーション開発",
    "title": "オークマ株式会社",
    "section": "1. Windows7 向け対話形式 3D-CAD/CAM アプリケーション開発",
    "text": "1. Windows7 向け対話形式 3D-CAD/CAM アプリケーション開発\n\n1.1. 期間\n\n2017/04 - 2018/07\n\n\n\n1.2. 担当業務\n\n大型アップデート用UI要件定義\nテスト仕様書作成\n結合テスト・システムテスト実施\nユーザ問い合わせ対応\n不具合対応\n\n\n\n1.3. 担当フェーズ\n\n要件定義\nテスト\n保守\n\n\n\n1.4. 開発環境\n\nWindows7\nC++\n\n\n\n1.5. 規模・役割\n\n10名\nメンバー",
    "crumbs": [
      "職務経歴 詳細",
      "オークマ株式会社"
    ]
  },
  {
    "objectID": "details/01-okuma.html#windows7-向け-2d-cadcam-アプリケーション開発",
    "href": "details/01-okuma.html#windows7-向け-2d-cadcam-アプリケーション開発",
    "title": "オークマ株式会社",
    "section": "2. Windows7 向け 2D-CAD/CAM アプリケーション開発",
    "text": "2. Windows7 向け 2D-CAD/CAM アプリケーション開発\n\n2.1. 期間\n\n2017/04 - 2018/07\n\n\n\n2.2. 担当業務\n\nテスト仕様書作成\n結合テスト・システムテスト実施\nユーザ問い合わせ対応\n受注先の工作機械に応じたチューニング\n\n\n\n2.3. 担当フェーズ\n\nテスト\n保守\n\n\n\n2.4. 開発環境\n\nWindows7\nC++\n\n\n\n2.5. 規模・役割\n\n5名\nメンバー",
    "crumbs": [
      "職務経歴 詳細",
      "オークマ株式会社"
    ]
  }
]